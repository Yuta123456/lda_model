{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.append(\"fashion-clip/\")\n",
    "from fashion_clip.fashion_clip import FashionCLIP\n",
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "fclip = FashionCLIP('fashion-clip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_csv('D:\\M1/fashion/lda_model/data/anotation_new.csv')\n",
    "img_path = annotations.iloc[:, 0]\n",
    "images = img_path.values[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'transform'=<function FashionCLIP.encode_images.<locals>.transform_fn at 0x000001752044E310> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "1it [00:17, 17.12s/it]\n"
     ]
    }
   ],
   "source": [
    "item_set = set()\n",
    "new_images = []\n",
    "for i in images:\n",
    "    item_id = i.split('/')[-1]\n",
    "    if item_id in item_set:\n",
    "        continue\n",
    "    new_images.append(i)\n",
    "    item_set.add(item_id)\n",
    "images = new_images\n",
    "images = [i.replace('\\\\', '/') for i in images]\n",
    "image_embeddings = fclip.encode_images(images, batch_size=128)\n",
    "image_embeddings = image_embeddings/np.linalg.norm(image_embeddings, ord=2, axis=-1, keepdims=True)\n",
    "category = ['Vest_top', 'Hair/alice_band', 'Leggings/Tights', 'T-shirt', 'Sneakers',\n",
    " 'Sunglasses', 'Cardigan', 'Gloves', 'Underwear_Tights', 'Hoodie', 'Other_shoe',\n",
    " 'Shorts' ,'Jumpsuit/Playsuit', 'Dress', 'Trousers', 'Belt', 'Socks',\n",
    " 'Underwear_bottom' ,'Bodysuit' ,'Hat/beanie', 'Scarf' ,'Jacket',\n",
    " 'Other_accessories', 'Bra' ,'Swimwear_bottom' ,'Blazer' ,'Top' ,'Polo shirt',\n",
    " 'Sweater' ,'Necklace' ,'Pyjama_set' ,'Blouse', 'Bag', 'Shirt' ,'Coat' ,'Boots',\n",
    " 'Skirt', 'Garment_Set', 'Bikini_top', 'Sandals' ,'Dungarees', 'Earring',\n",
    " 'Cap/peaked' ,'Ballerinas', 'Swimsuit' ,'Hat/brim']\n",
    "materials = ['Cotton', 'Polyester', 'Rayon', 'Linen', 'Wool', 'Silk', 'Nylon', 'Polyurethane', 'Denim', 'Spandex']\n",
    "colors = ['Black', 'White', 'Gray', 'Navy', 'Blue', 'Red', 'Green', 'Brown', 'Beige', 'Pink',\n",
    "          'Yellow', 'Orange', 'Purple', 'Lavender', 'Teal', 'Turquoise', 'Magenta', 'Olive', 'Maroon', 'Charcoal']\n",
    "created_material_labels = []\n",
    "created_color_labels = []\n",
    "for c in category:\n",
    "    for color in colors:\n",
    "        created_color_labels.append(f'{c}_{color}')\n",
    "    for m in materials:\n",
    "        created_material_labels.append(f'{c}_{m}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:01, 12.23it/s]                        \n",
      "29it [00:02, 12.89it/s]                        \n"
     ]
    }
   ],
   "source": [
    "label_material_embeddings = fclip.encode_text(created_material_labels, batch_size=32)\n",
    "label_material_embeddings = label_material_embeddings/np.linalg.norm(label_material_embeddings, ord=2, axis=-1, keepdims=True)\n",
    "label_color_embeddings = fclip.encode_text(created_color_labels, batch_size=32)\n",
    "label_color_embeddings = label_color_embeddings/np.linalg.norm(label_color_embeddings, ord=2, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicteds\n",
    "predicted_materials = label_material_embeddings.dot(image_embeddings.T)\n",
    "predicted_materials = [created_material_labels[k] for k in np.argmax(predicted_materials, axis=0)]\n",
    "predicted_colors = label_color_embeddings.dot(image_embeddings.T)\n",
    "predicted_colors = [created_color_labels[k] for k in np.argmax(predicted_colors, axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\M1/fashion/lda_model/data/fashion_clip_new.txt', 'w') as f:\n",
    "    for img_path, m, c in zip(images, predicted_materials, predicted_colors):\n",
    "        # item_id = i.split('/')[-1]\n",
    "        item_id = img_path.split('/')[-1][:-6]\n",
    "        f.write(f'{item_id}, {m}, {c} \\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
